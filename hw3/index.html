<html>
	<head>
		<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=default'></script>
		<link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600&display=swap" rel="stylesheet">
		<style>
			h1 {
				text-align: center;
			}

			.container {
				margin: 0 auto;
				padding: 60px 20%;
			}

			figure {
				text-align: center;
			}

			img {
				display: inline-block;
			}

			body {
				font-family: 'Inter', sans-serif;
			}
		</style>
	</head>
	<body>
		<div class="container">
		<h1>CS184/284A Spring 2025 Homework 3 Write-Up</h1>
		<div style="text-align: center;">Name: Tara Pande</div>

		<br>

		Link to webpage: <a href="https://cal-cs184-student.github.io/hw-webpages-tara/">cal-cs184-student.github.io/hw-webpages-tara/</a>
		Link to GitHub repository: <a href="https://github.com/cal-cs184-student/sp25-hw3-tarapande3">github.com/cal-cs184-student/sp25-hw3-tarapande3</a>
		
		<figure>
			<img src="cornell.png" alt="Cornell Boxes with Bunnies" style="width:70%"/>
			<figcaption>Light Sampling Techniques</figcaption>
		</figure>

		<!--
		We've already added one heading per part, to make your write-up as navigable when grading. Please fit your write-up within these sections!
		-->

		<h2>Overview</h2>
		Give a high-level overview of what you implemented in this homework. Think about what you've built as a whole. Share your thoughts on what interesting things you've learned from completing the homework.

		<h2>Part 1: Ray Generation and Scene Intersection</h2>
		<b>(A)</b> The rendering pipeline in the raytracer has two main components: ray generation and primitive intersection. For ray generation, first we map the normalized (x,y) image coordinates we are given to a sensor plane (\(X_c, Y_c, Z_c\)) within the camera space. The formula to do this is:

		\[(X_c, Y_c, Z_c) = ((2x-1) • \tan(hFov/2), (2y-1) • \tan(vFov/2), -1\]

		where hFov and vFov are the field of view angles that define the sensor, converted from degrees to radians. 

		Next, we need to create the ray in camera space. The ray's origin is at the camera's position (0,0,0). Therefore, we can simple compute the ray direction to be the normalized vector: 

		\[dir_c = (X_c, Y_c, Z_c)\]

		Then we need to multiply this ray direction by the camera-2-world transformation matrix that rotates the ray vector from camera space to world space. In this case, the ray's origin is the pos variable.

		Lastly, we set ray.min_t = nClip and ray.max_t = fClip which defines the range of distances within which objects are visible.

		Once the camera rays are generated, we test for intersections using scene primatives like triangles and spheres. For ray-triangle intersection, we use barycentric coordinates to check if the ray hits a triangle by computing the intersection point P and checking if it lies inside the triangle. For ray-sphere intersection we use the quadratic equation to solve for the intersection point(s) t (choosing the closest one that lies within min_t and max_t). Using this information, we are able to compute the color at a pixel by tracing multiple rays per pixel and averaging their radiance estimates.

		<b>(B)</b> I used the Moller-Trumbore algorithm for this task. First, I defined the triangle with three vertices p1, p2, and p3. With these vertices, we can compute two edge vectors that define the plane of the triangle. Then, I compute the determinant to check for parallelism: if dot(edge1, cross(ray\_direction, edge2)) is super close to 0, then the ray is parallel the triangle and no intersection occurs.

		Next, I compute the barycentric coordinates u and v:

		\[u=1/determinant • dot(ray\_origin - p1, cross(ray\_direction, edge2))\] 

		\[v = 1/determinant • dot(ray\_direction, cross(ray\_origin - p1, edge1))\]

		If either u or v lies outside of [0, 1], then the intersection point lies outside of the triangle. Next, I need to compute the distance along the ray where the intersection occurs:

		\[dist = f • dot(edge2, cross(ray\_origin - p1, edge1))\]

		In this case, if dist is outside of [min_t, max_t, then the intersection is not within the visible range of the camera.]

		Finally, if there is a valid intersection, we compute the normalized interpolated normal n using barycentric coordinates:

		\[n = (1 - u - v) n_1 + un_2 + vn_3\]

		and store the new max_t, intersection primitive, interpolated normal, and bsdf values to intersection class before returning true (there is an intersection).

		<p><b>(C)</b> Here are four examples of images with normal shading:</p>
		<div style="display: flex; flex-direction: column; align-items: center;">
			<table style="width: 100%; text-align: center; border-collapse: collapse;">
			  <tr>
				<td style="text-align: center;">
				  <img src="part1/banana.png" width="400px"/>
				  <figcaption>Banana</figcaption>
				</td>
				<td style="text-align: center;">
				  <img src="part1/CBspheres.png" width="400px"/>
				  <figcaption>Spheres</figcaption>
				</td>
			  </tr>
			  <tr>
				<td style="text-align: center;">
				  <img src="part1/cow.png" width="400px"/>
				  <figcaption>Cow</figcaption>
				</td>
				<td style="text-align: center;">
				  <img src="part1/teapot.png" width="400px"/>
				  <figcaption>Teapot</figcaption>
				</td>
			  </tr>
			</table>
		</div>
		
		<h2>Part 2: Bounding Volume Hierarchy</h2>
		<b>(A)</b> BVH is a binary tree algorithm that accelerates ray tracing by organizing parts of geometric objects into a tree structure to make checking for intersections faster. 

		Here is how my BVH construction algorithm works. First, I interate over all of the primitives from start to end and compute a bounding box that encompasses all of them. Next, I check the leaf node condition: if the number of primitives is less than or equal to max_leaf_size, the node should be designated as a leaf node. Now, I need to determine which axis to split along based on the largest extent of the bounding box. I chose to split along the longest axis specifically for my heuristic because it helps create a more balanced tree and reduces the tree's depth. Next, I compute the centroid average along the longest axis. I used the mean centroid position as my split point heuristic because it will allow for a more even distribution of the primitives. Finally, I partition the primitives into left and right sets based on whether their centroid is smaller or larger than the split point, and then continue recursively building the left and right subtrees.

		Ultimately, this allows for a much faster traversal to check for ray intersections, going from O(n) time to O(log(n)) time.


		<p><b>(B)</b> Here are four larger images with normal shading that utilize BVH acceleration:</p>
		<div style="display: flex; flex-direction: column; align-items: center;">
			<table style="width: 100%; text-align: center; border-collapse: collapse;">
			  <tr>
				<td style="text-align: center;">
				  <img src="part2/CBlucy.png" width="400px"/>
				  <figcaption>Lucy</figcaption>
				</td>
				<td style="text-align: center;">
				  <img src="part2/dragon.png" width="400px"/>
				  <figcaption>Dragon</figcaption>
				</td>
			  </tr>
			  <tr>
				<td style="text-align: center;">
				  <img src="part2/maxplanck.png" width="400px"/>
				  <figcaption>Max Planck</figcaption>
				</td>
				<td style="text-align: center;">
				  <img src="part2/wall-e.png" width="400px"/>
				  <figcaption>Wall-E</figcaption>
				</td>
			  </tr>
			</table>
		</div>

		<b>(C)</b> Without BVH acceleration, rendering times ultimately scale poorly for complex scenes due to the O(n) complexity of brute-force ray intersection tests. In this case, each ray would need to test against every primitive, significantly increasing computation time. All four of these images above have anywhere from roughly 100k to 250k primitives. In my testing, I've noticed that BVH reduces the rendering time by 64% (Dragon: ~110k primitives) to 91% (Wall-E: ~240k primitives) on my laptop as it can efficiently cull unnecessary intersection tests with logarithic traversal, making real-time rendering feasible for these complex scenes. 


		<h2>Part 3: Direct Illumination</h2>
		<b>(A)</b> There are two different implementations of the direct lighting function: uniform hemisphere sampling (estimates direct lighting by uniformly sampling the hemisphere around the intersection point) and importance sampling (importance sampling light sources instead of random hemisphere directions which improves efficiency).

		For uniform hemisphere sampling, I use a local coordinate system created by aligning the surface normal with the Z-direction because this helps converting sampled hemisphere directions into world space. For my sampling loop, I generate random directions w_in uniformly in the hemisphere. Then, I cast a "shadow ray" in the sample direction (origin at hit_p • w_in, and cast to w_in) check if it reaches a light source or not. If the ray hits a light source, the emitted light is accumulated using a Monte Carlo estimator which is made up of the Bidirectional Scattering Distribution Function (BSDF), cosine term (accounts for light directionality), and hemisphere probability density function (PDF):

		\[L\_out += (BSDF • emitted\_light • cosine) / PDF\] 

		At the end, it is important to normalize L\_out by dividing it by the total number of directions sampled within the hemisphere.

		For importance sampling sampling loop, first I iterate over all the scene lights, processing them individually. I use the light's sample_L method to determine its direction wi and distance. Then I cast a shadow ray from hit_p towards wi and check for intersections. If the path is clear (not blocked), it accumulates radiance using BSDF and the sampled light's contribution. In this case, the sample is already weighted based on the light distribution, but I still normalize L\_out by dividing by the number of samples. Ultimately, I compute the reflected radiance (L\_out) as:

		\[L\_out += (L\_in • BSDF • cosine) / PDF\]

		Ultimately, both of these functions compute one bounce radiance. When added to the BSDF's emission (coming from the light source directly = zero bounce radiance), we get the images like you see below.

		<p><b>(B)</b> Here are some side by side comparisons of 2 images rendered using the direct lighting function with uniform hemisphere sampling (left) and the direct lighting function with importance sampling (right).</p>
		<div style="display: flex; flex-direction: column; align-items: center;">
			<table style="width: 100%; text-align: center; border-collapse: collapse;">
			  <!-- <tr>
				<td style="text-align: center;">
				  <img src="part3/building_H_16_8.png" width="400px"/>
				  <figcaption>Building (hemisphere)</figcaption>
				</td>
				<td style="text-align: center;">
				  <img src="part3/building_16_8.png" width="400px"/>
				  <figcaption>Building (importance)</figcaption>
				</td>
			  </tr> -->
			  <tr>
				<td style="text-align: center;">
				  <img src="part3/CBbunny_H_16_8.png" width="400px"/>
				  <figcaption>Bunny (hemisphere)</figcaption>
				</td>
				<td style="text-align: center;">
				  <img src="part3/CBbunny_16_8.png" width="400px"/>
				  <figcaption>Bunny (importance)</figcaption>
				</td>
			  </tr>
			  <tr>
				<td style="text-align: center;">
				  <img src="part3/CBspheres_H_16_8.png" width="400px"/>
				  <figcaption>Spheres (hemisphere)</figcaption>
				</td>
				<td style="text-align: center;">
				  <img src="part3/CBspheres_16_8.png" width="400px"/>
				  <figcaption>Spheres (importance)</figcaption>
				</td>
			  </tr>
			</table>
		</div>

		<p><b>(C)</b> Here is one particular scene showing a comparison of noise levels in soft shadows when rendered with 1, 4, 16, and 64 light rays using light sampling.</p>
		<div style="display: flex; flex-direction: column; align-items: center;">
			<table style="width: 100%; text-align: center; border-collapse: collapse;">
			  <tr>
				<td style="text-align: center;">
				  <img src="part3/CBspheres_l1.png" width="400px"/>
				  <figcaption>Spheres (1 light ray)</figcaption>
				</td>
				<td style="text-align: center;">
				  <img src="part3/CBspheres_l4.png" width="400px"/>
				  <figcaption>Spheres (4 light rays)</figcaption>
				</td>
			  </tr>
			  <tr>
				<td style="text-align: center;">
				  <img src="part3/CBspheres_l16.png" width="400px"/>
				  <figcaption>Spheres (16 light rays)</figcaption>
				</td>
				<td style="text-align: center;">
				  <img src="part3/CBspheres_l64.png" width="400px"/>
				  <figcaption>Spheres (64 light rays)</figcaption>
				</td>
			  </tr>
			</table>
		</div>

		<b>(D)</b> When comparing the results between uniform hemisphere sampling and lighting sampling, ultimately uniform hemisphere sampling distributes samples evenly in all directions, often resulting in high variance since many sampled directions do not contribute to the lighting. In my results, this has led to noisy images (especially when the light source occupies a small solid angle). 

		On the other hand, importance sampling is able to focus on directions where actual light sources exist, which significantly improves the efficiency and reduces noise, ultimately producing higher-quality results with fewer samples. However, this technique does require explicit knowledge of the light positions.


		<h2>Part 4: Global Illumination</h2>
		<b>(A)</b> First, I will explain how I implemented the indirect lighting function. The first step of the implementation is to use sample_f to sample an incoming direction w\_in from the BSDF. Then, setting my Russian Roulette termination probability to 0.7, I use the coin\_flip() function to determine whether we should continue to recurse more light bounces (this method ensures that rays don't infinitely recurse). Also, if the ray depth equal the max_ray_depth, we force recursion to ensure at least one indirect bounce. Next, I convert w\_in from object to world coordinates using the object-2-world transformation matrix, create a new ray originating at hit\_p • w\_in through w\_in, and then reduce the ray depth by 1 as we recurse. Next, I check if the new ray hits another object. If so, I recursively compute indirect illumination in order to finally calculate and return L\_out as follows:

		\[L\_out = (indirect • BSDF • cosine) / (PDF • RR\_prob)\]

		<b>(B)</b> Here are two images rendered with global (direct and indirect illumination) using 1024 samples per pixel:
		<div style="display: flex; flex-direction: column; align-items: center;">
			<table style="width: 100%; text-align: center; border-collapse: collapse;">
			  <tr>
				<td style="text-align: center;">
				  <img src="part4/CBspheres_global.png" width="400px"/>
				  <figcaption>Spheres</figcaption>
				</td>
				<td style="text-align: center;">
				  <img src="part4/CBgems_global.png" width="400px"/>
				  <figcaption>Gems (object doesn't use BSDFs)</figcaption>
				</td>
			  </tr>
			</table>
		</div>

		<b>(C)</b> Here is a direct comparison of direct vs indirect sampling on a scene using 1024 samples per pixel:
		<div style="display: flex; flex-direction: column; align-items: center;">
			<table style="width: 100%; text-align: center; border-collapse: collapse;">
			  <tr>
				<td style="text-align: center;">
				  <img src="part4/CBspheres_direct.png" width="400px"/>
				  <figcaption>Direct Illumination Only</figcaption>
				</td>
				<td style="text-align: center;">
				  <img src="part4/CBspheres_indirect.png" width="400px"/>
				  <figcaption>Indirect Illumination Only</figcaption>
				</td>
			  </tr>
			</table>
		</div>

		<p><b>(D)</b> Here are my renderings of CBbunny.dae with the max ray depth set to 0, 1, 2, 3, 4, and 5 bounces where isAccumBounces = false (using 1024 samples per pixel):</p>

		What I find super interesting is that between the 2nd and 3rd bounce of light, the ceiling goes from completely black to being slightly lit up which is ultimately much more realistic as ceiling lights in real life also light up the ceiling, not just the walls, floor, and objects in the room. This shows why indirect lighting is so important. Using rasterization, we only consider direct lighting (what surfaces can directly see the light source), but this causes unrealistic color transitions since light bounces off many surfaces in the real world as just explained: in the real world, even if a surface doesn't have direct access to a light source, it isn't completely pitch black, it is just shadowed slightly. Indirect lighting accounts for this the more bounces you have (need at least 3 in this case), but rasterization does not consider this at all.

		Also, when comparing accumulated vs unaccumulated bounces for CBbunny.dae across different max ray depths, I notice that having accumulated bounces brightens up the image a lot and incorporates the light source's zero bounce radiance. The number of max ray depth is helpful to get the light traveling and bouncing throughout the room, but setting accumulated bounces to true allows for the light to accumulate in intensity and brighten up the image in a much more realistic way. Also, of course we need the ceiling light to be not a black box!

		<p><b>(E)</b> Here are my Russian Roulette renderings across different max ray depths (0, 1, 2, 3, 4, and 100) using 1024 samples per pixel:</p>

		<p><b>(F)</b> Here is one scene that shows a comparison of rendered views with various sample-per-pixel rates using 4 light rays:</p>
		<div style="display: flex; flex-direction: column; align-items: center;">
			<table style="width: 100%; text-align: center; border-collapse: collapse;">
			  <tr>
				<td style="text-align: center;">
				  <img src="part4/CBspheres_s1.png" width="400px"/>
				  <figcaption>1 Sample-Per-Pixel</figcaption>
				</td>
				<td style="text-align: center;">
				  <img src="part4/CBspheres_s2.png" width="400px"/>
				  <figcaption>2 Samples-Per-Pixel</figcaption>
				</td>
			  </tr>
			  <tr>
				<td style="text-align: center;">
				  <img src="part4/CBspheres_s4.png" width="400px"/>
				  <figcaption>4 Samples-Per-Pixel</figcaption>
				</td>
				<td style="text-align: center;">
				  <img src="part4/CBspheres_s8.png" width="400px"/>
				  <figcaption>8 Samples-Per-Pixel</figcaption>
				</td>
			  </tr>
			  <tr>
				<td style="text-align: center;">
				  <img src="part4/CBspheres_s16.png" width="400px"/>
				  <figcaption>16 Samples-Per-Pixel</figcaption>
				</td>
				<td style="text-align: center;">
				  <img src="part4/CBspheres_s64.png" width="400px"/>
				  <figcaption>64 Samples-Per-Pixel</figcaption>
				</td>
			  </tr>
			  <tr>
				<td style="text-align: center;">
				  <img src="part4/CBspheres_s1024.png" width="400px"/>
				  <figcaption>1024 Samples-Per-Pixel</figcaption>
				</td>
			  </tr>
			</table>
		</div>

		<!-- <b>(G)</b> I made all of my renders on my laptop. -->


		<h2>Part 5: Adaptive Sampling</h2>
		Explain adaptive sampling. Walk through your implementation of the adaptive sampling.

		<b>(A)</b> Adaptive sampling is an optimization technique used to reduce the number of samples required to converge to a noise-free solution in Monte Carlo path tracing. Ultimately, we want to focus more samples on areas of the image that have high variance ore require more samples to reduce noise (while using fewer samples in areas where the pixel values have already converged).

		Here is how my adaptive sampling implementation works. First, we accumulate the illuminance and radiance across our intial sampling. After a certain number of samples, we then calculate the mean and standard deviation of the sample's illuminance. From here, we can compute a confidence interval for the pixel's average radiance. But if the confidence interval is small enough, we can determine that the pixel has converged and to stop sampling for that pixel in the future. In this case, the stop condition is:

		\[I = 1.96 • standard\_deviation / sqrt(current\_sample\_num)\] 

		If I is less than or equal to the maximum tolerance times the mean illuminance, then the pixel is converged. If the pixel has not converged, we continue sampling and updating the average radiance statistics until it does. 

		<p><b>(B)</b> Here are two scenes rendered with 2048 samples per pixel that clearly show visible differences in sampling rate over various regions and pixels:</p>
		<div style="display: flex; flex-direction: column; align-items: center;">
			<table style="width: 100%; text-align: center; border-collapse: collapse;">
			  <tr>
				<td style="text-align: center;">
				  <img src="part5/banana_as.png" width="400px"/>
				  <figcaption>Banana (Noise-free Rendered Result)</figcaption>
				</td>
				<td style="text-align: center;">
				  <img src="part5/banana_as_rate.png" width="400px"/>
				  <figcaption>Banana (Sample Rate Image)</figcaption>
				</td>
			  </tr>
			  <tr>
				<td style="text-align: center;">
				  <img src="part5/CBspheres_as.png" width="400px"/>
				  <figcaption>Spheres (Noise-free Rendered Result)</figcaption>
				</td>
				<td style="text-align: center;">
				  <img src="part5/CBspheres_as_rate.png" width="400px"/>
				  <figcaption>Spheres (Sample Rate Image)</figcaption>
				</td>
			  </tr>
			</table>
		</div>


		<!-- <h2>(Optional) Part 6: Extra Credit Opportunities</h2>
		Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum. -->
		
		<!-- <h2>Additional Notes (please remove)</h2>
		<ul>
			<li>You can also add code if you'd like as so: <code>code code code</code></li>
			<li>If you'd like to add math equations, 
				<ul>
					<li>You can write inline equations like so: \( a^2 + b^2 = c^2 \)</li>
					<li>You can write display equations like so: \[ a^2 + b^2 = c^2 \]</li>
				</ul>
			</li>
		</ul> -->
		</div>
	</body>
</html>